{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the following cell (It takes almost an hour to complete, depending on your computer power):",
   "id": "a8978bb870f3f61d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import spacy\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load spaCy model for Named Entity Recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load your data\n",
    "file_path = 'america_north.Canada.eng.1'  # Read the file.\n",
    "df = pd.read_csv(file_path, usecols=['Index', 'City', 'Text'])\n",
    "\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "\n",
    "# Define regions\n",
    "regions = {\n",
    "    'Atlantic Canada': ['fredericton', 'saint john', 'moncton', 'halifax', 'charlottetown', 'new glasgow', 'corner brook', 'truro', 'glace bay'],\n",
    "    'Quebec': ['montreal', 'quebec', 'sherbrooke', 'trois-rivieres', 'levis', 'saint-hyacinthe', 'saint-jean-sur-richelieu', 'saint-jerome', 'shawinigan', 'drummondville', 'granby', 'saint-laurent', 'salaberry-de-valleyfield', 'sorel-tracy', 'victoriaville', 'rouyn-noranda', 'sept-iles', 'alma', 'joliette', 'brossard'],\n",
    "    'Ontario': ['toronto', 'ottawa', 'hamilton', 'london', 'kingston', 'windsor', 'oshawa', 'barrie', 'guelph', 'kitchener', 'waterloo', 'brantford', 'st. catharines', 'peterborough', 'sudbury', 'thunder bay', 'timmins', 'north bay', 'sault ste. marie', 'stratford', 'orillia', 'brockville', 'chatham-kent', 'sarnia', 'woodstock', 'owen sound', 'midland', 'cobourg', 'orangeville', 'cornwall', 'leamington', 'bradford west gwillimbury', 'grand sudbury'],\n",
    "    'Prairie Provinces': ['calgary', 'edmonton', 'regina', 'saskatoon', 'winnipeg', 'lethbridge', 'red deer', 'medicine hat', 'grande prairie', 'moose jaw', 'prince albert', 'brandon', 'airdrie', 'spruce grove', 'lloydminster', 'north battleford', 'sherwood park'],\n",
    "    'British Columbia': ['vancouver', 'victoria', 'kelowna', 'kamloops', 'nanaimo', 'prince george', 'abbotsford', 'chilliwack', 'vernon', 'penticton', 'campbell river', 'courtenay', 'port alberni', 'parksville', 'duncan', 'terrace', 'cranbrook', 'white rock', 'new westminster', 'north vancouver', 'richmond', 'north cowichan'],\n",
    "    'Northern Territories': ['whitehorse'],\n",
    "}\n",
    "\n",
    "# Function to map cities to regions\n",
    "def map_to_region(city):\n",
    "    city = city.lower()\n",
    "    for region, cities in regions.items():\n",
    "        if city in cities:\n",
    "            return region\n",
    "    return 'Unknown'  # Changed from 'Others' to 'Unknown'\n",
    "\n",
    "# Apply the mapping\n",
    "print(\"\\nMapping cities to regions...\")\n",
    "df['Region'] = df['City'].apply(map_to_region)\n",
    "\n",
    "print(\"\\nOriginal regional distribution:\")\n",
    "print(df['Region'].value_counts())\n",
    "\n",
    "# After applying the mapping\n",
    "print(\"\\nCities not assigned to a region:\")\n",
    "unassigned_cities = df[df['Region'] == 'Unknown']['City'].unique()\n",
    "print(unassigned_cities)\n",
    "\n",
    "# Define the number of samples per region\n",
    "samples_per_region = 200000  # Increased to 200,000 for better performance\n",
    "\n",
    "# Sample equally from each region\n",
    "sampled_df = pd.DataFrame()\n",
    "for region in df['Region'].unique():\n",
    "    if region != 'Unknown':  # Exclude 'Unknown' region from sampling\n",
    "        region_df = df[df['Region'] == region]\n",
    "        if len(region_df) > samples_per_region:\n",
    "            sampled_region = region_df.sample(n=samples_per_region, random_state=42)\n",
    "        else:\n",
    "            sampled_region = region_df  # If a region has fewer samples, take all of them\n",
    "        sampled_df = pd.concat([sampled_df, sampled_region])\n",
    "\n",
    "print(\"\\nSampled dataset shape:\", sampled_df.shape)\n",
    "print(\"\\nSampled regional distribution:\")\n",
    "print(sampled_df['Region'].value_counts())\n",
    "\n",
    "# Replace the original dataframe with the sampled one\n",
    "df = sampled_df\n",
    "\n",
    "# Data cleaning function with progress reporting\n",
    "def clean_text_with_progress(texts):\n",
    "    is_series = isinstance(texts, pd.Series)\n",
    "    if is_series:\n",
    "        total = len(texts)\n",
    "        cleaned_texts = texts.copy()\n",
    "    else:\n",
    "        total = len(texts)\n",
    "        cleaned_texts = texts.copy()\n",
    "\n",
    "    for step in range(1, 7):\n",
    "        print(f\"\\nStep {step}/6: \", end=\"\")\n",
    "        if step == 1:\n",
    "            print(\"Converting to lowercase...\")\n",
    "            if is_series:\n",
    "                cleaned_texts = cleaned_texts.progress_apply(lambda x: x.lower())\n",
    "            else:\n",
    "                cleaned_texts = [x.lower() for x in tqdm(cleaned_texts, total=total, ncols=70)]\n",
    "        elif step == 2:\n",
    "            print(\"Removing punctuation...\")\n",
    "            if is_series:\n",
    "                cleaned_texts = cleaned_texts.progress_apply(\n",
    "                    lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "            else:\n",
    "                cleaned_texts = [x.translate(str.maketrans('', '', string.punctuation)) for x in\n",
    "                                 tqdm(cleaned_texts, total=total, ncols=70)]\n",
    "        elif step == 3:\n",
    "            print(\"Tokenizing...\")\n",
    "            if is_series:\n",
    "                cleaned_texts = cleaned_texts.progress_apply(nltk.word_tokenize)\n",
    "            else:\n",
    "                cleaned_texts = [nltk.word_tokenize(x) for x in tqdm(cleaned_texts, total=total, ncols=70)]\n",
    "        elif step == 4:\n",
    "            print(\"Removing stopwords...\")\n",
    "            stopword_list = set(stopwords.words(\"english\"))\n",
    "            if is_series:\n",
    "                cleaned_texts = cleaned_texts.progress_apply(\n",
    "                    lambda x: [word for word in x if word not in stopword_list])\n",
    "            else:\n",
    "                cleaned_texts = [[word for word in x if word not in stopword_list] for x in\n",
    "                                 tqdm(cleaned_texts, total=total, ncols=70)]\n",
    "        elif step == 5:\n",
    "            print(\"Stemming...\")\n",
    "            stemmer = PorterStemmer()\n",
    "            if is_series:\n",
    "                cleaned_texts = cleaned_texts.progress_apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "            else:\n",
    "                cleaned_texts = [[stemmer.stem(word) for word in x] for x in tqdm(cleaned_texts, total=total, ncols=70)]\n",
    "        else:\n",
    "            print(\"Joining tokens...\")\n",
    "            if is_series:\n",
    "                cleaned_texts = cleaned_texts.progress_apply(lambda x: \" \".join(x))\n",
    "            else:\n",
    "                cleaned_texts = [\" \".join(x) for x in tqdm(cleaned_texts, total=total, ncols=70)]\n",
    "\n",
    "        time.sleep(0.5)  # Add a small delay between steps\n",
    "\n",
    "    return cleaned_texts\n",
    "\n",
    "# Apply cleaning to the Text column\n",
    "print(\"\\nCleaning text data...\")\n",
    "df['CleanedText'] = clean_text_with_progress(df['Text'])\n",
    "\n",
    "# Feature extraction classes\n",
    "class CityNameExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, city_list):\n",
    "        self.city_list = city_list\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        features = [[1 if city in text.lower() else 0 for city in self.city_list] for text in X]\n",
    "        return csr_matrix(features)\n",
    "\n",
    "class NamedEntityExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        loc_features = self._extract_locations(X)\n",
    "        self.vectorizer.fit(loc_features)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        loc_features = self._extract_locations(X)\n",
    "        return self.vectorizer.transform(loc_features)\n",
    "\n",
    "    def _extract_locations(self, X):\n",
    "        loc_features = []\n",
    "        for text in tqdm(X, desc=\"Extracting Named Entities\"):\n",
    "            doc = nlp(text)\n",
    "            locations = [ent.text.lower() for ent in doc.ents if ent.label_ == 'GPE']\n",
    "            loc_features.append(' '.join(locations))\n",
    "        return loc_features\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = df['CleanedText']\n",
    "y = df['Region']\n",
    "\n",
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create a list of all cities\n",
    "all_cities = [city.lower() for cities in regions.values() for city in cities]\n",
    "\n",
    "# Create the feature extraction pipeline\n",
    "feature_extractor = FeatureUnion([\n",
    "    ('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1, 2))),\n",
    "    ('city_names', CityNameExtractor(all_cities)),\n",
    "    ('city_names_2', CityNameExtractor(all_cities)),\n",
    "    ('city_names_3', CityNameExtractor(all_cities)),  # Added third instance to increase weight\n",
    "    ('named_entities', NamedEntityExtractor())\n",
    "])\n",
    "\n",
    "# Create the full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('features', feature_extractor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))  # Changed to GradientBoostingClassifier\n",
    "])\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Example of predicting new texts\n",
    "new_texts = [\n",
    "    \"The CN Tower offers a spectacular view of Toronto.\",\n",
    "    \"Vancouver's Stanley Park is a beautiful urban oasis.\",\n",
    "    \"Montreal's old town is full of historic charm.\",\n",
    "    \"The Calgary Stampede is a world-famous rodeo event.\",\n",
    "    \"The small town charm of Fredericton is unforgettable.\",\n",
    "    \"Whitehorse is the gateway to Yukon's wilderness.\",\n",
    "    \"The prairies stretch as far as the eye can see in Saskatchewan.\"\n",
    "]\n",
    "\n",
    "# Clean new texts\n",
    "print(\"\\nCleaning new texts...\")\n",
    "cleaned_new_texts = clean_text_with_progress(new_texts)\n",
    "\n",
    "# Predict\n",
    "probabilities = pipeline.predict_proba(cleaned_new_texts)\n",
    "\n",
    "print(\"\\nPredictions for new texts:\")\n",
    "for original_text, cleaned_text, probs in zip(new_texts, cleaned_new_texts, probabilities):\n",
    "    print(f\"Original Text: '{original_text}'\")\n",
    "    print(f\"Cleaned Text: '{cleaned_text}'\")\n",
    "    for region, prob in sorted(zip(le.classes_, probs), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{region}: {prob * 100:.2f}%\")\n",
    "    print()"
   ],
   "id": "9d37a5319a27675f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
